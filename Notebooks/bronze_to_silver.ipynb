{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe10734d-57e8-4e01-bcbe-95702834b3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuração do Ambiente\n",
    "\n",
    "**Propósito:** Inicializar o ambiente PySpark e configurar o contexto do catálogo Unity Catalog para a camada Bronze → Silver.\n",
    "\n",
    "**Entrada:**\n",
    "- Nenhuma tabela (apenas configuração inicial)\n",
    "\n",
    "**Saída:**\n",
    "- Variáveis de ambiente: `catalog_name`, `bronze_db_name`, `silver_db_name`\n",
    "- Contexto Spark configurado para o schema Silver\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Importa todas as funções PySpark necessárias para transformações de dados (normalização de texto, funções de janela, fuzzy matching com Levenshtein, timestamps). Define os nomes do catálogo e schemas Bronze/Silver, e configura o Spark para usar o schema Silver por padrão.\n",
    "\n",
    "**Observações:**\n",
    "- O catálogo usado é `atendimento_catalog` (Unity Catalog)\n",
    "- Todas as transformações operam entre `bronze` (source) e `silver` (target)\n",
    "- Levenshtein será usado para fuzzy matching de motivos de chamados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7c7950-7e18-4905-95b4-ba33b8790f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, lower, initcap, current_timestamp,\n",
    "    lit, coalesce, when, regexp_replace, length, row_number, to_timestamp, levenshtein, unix_timestamp, round\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "catalog_name = \"atendimento_catalog\"\n",
    "bronze_db_name = \"bronze\"\n",
    "silver_db_name = \"silver\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377f5eab-2e59-42ca-8e8f-4ed6ef074732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver_db_name}\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {silver_db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1fe91d-32c5-400e-ad4a-1475ed1547b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Funções Auxiliares de Transformação\n",
    "\n",
    "**Propósito:** Definir funções reutilizáveis para validação, normalização de texto, conversão de tipos e parsing de timestamps.\n",
    "\n",
    "**Entrada:**\n",
    "- Nenhuma tabela (apenas definições de funções)\n",
    "\n",
    "**Saída:**\n",
    "- Funções Python disponíveis para uso nas transformações:\n",
    "  - `safe_table_exists`: Verifica se uma tabela existe no catálogo\n",
    "  - `safe_col`: Retorna coluna ou `NULL` se não existir\n",
    "  - `safe_cast_int`: Converte para inteiro removendo caracteres não-numéricos\n",
    "  - `remove_accents_udf`: Remove acentuação de texto (Unicode normalizado)\n",
    "  - `normalize_text`: Remove caracteres especiais e normaliza espaços\n",
    "  - `parse_to_brasilia_timezone`: Converte timestamps para timezone de São Paulo\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Encapsula lógica comum de validação e transformação usada em todas as tabelas Silver. As funções de normalização aplicam remoção de acentos, trim, regex para espaços e caracteres inválidos. O parsing de timestamp corrige formatos brasileiros típicos (dd/MM/yyyy às HH:mm:ss) e converte para timezone UTC com offset de São Paulo.\n",
    "\n",
    "**Observações:**\n",
    "- `safe_cast_int` é tolerante a falhas: retorna `NULL` se conversão falhar\n",
    "- `remove_accents_udf` preserva apenas caracteres ASCII normalizados (A-Z, a-z)\n",
    "- Timestamps são armazenados em UTC mas exibidos com timezone de São Paulo\n",
    "- Funções são idempotentes e NULL-safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63af09c6-021b-4f88-9869-b9bda470e3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def safe_table_exists(spark, full_name: str) -> bool:\n",
    "    try:\n",
    "        spark.table(full_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def safe_col(df, name):\n",
    "    return col(name) if name in df.columns else lit(None).cast(StringType())\n",
    "\n",
    "def safe_cast_int(col_expr):\n",
    "    return when(\n",
    "        col_expr.isNotNull() & (trim(col_expr).cast(StringType()) != \"\"),\n",
    "        F.regexp_replace(trim(col_expr).cast(StringType()), r'[^\\d]', '').cast(IntegerType())\n",
    "    ).otherwise(None)\n",
    "\n",
    "def remove_accents_udf(text_col):\n",
    "    return F.translate(\n",
    "        text_col,\n",
    "        \"áàãâäéèêëíìîïóòõôöúùûüçñÁÀÃÂÄÉÈÊËÍÌÎÏÓÒÕÔÖÚÙÛÜÇÑ\",\n",
    "        \"aaaaaeeeeiiiiooooouuuucnAAAAAEEEEIIIIOOOOOUUUUCN\"\n",
    "    )\n",
    "\n",
    "def normalize_text(col_expr):\n",
    "    return trim(regexp_replace(regexp_replace(col_expr, r'\\s+', ' '), r'[^\\x20-\\x7E\\u00C0-\\u00FF]', ''))\n",
    "\n",
    "def parse_to_brasilia_timezone(col_expr):\n",
    "    cleaned = regexp_replace(col_expr, r'[ ]', ' ')\n",
    "    cleaned = regexp_replace(cleaned, r'\\bàs\\b|\\bas\\b|\\bàs\\b', ' ')\n",
    "    cleaned = regexp_replace(cleaned, r'\\s+', ' ')\n",
    "    cleaned = trim(cleaned)\n",
    "    parsed_naive = to_timestamp(cleaned, 'dd/MM/yyyy HH:mm:ss')\n",
    "    parsed_utc = F.to_utc_timestamp(parsed_naive, 'America/Sao_Paulo')\n",
    "    return F.from_utc_timestamp(parsed_utc, 'America/Sao_Paulo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3839bbe2-df79-40b6-ab61-502328322aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformação: ft_atendentes (Bronze → Silver)\n",
    "\n",
    "**Propósito:** Normalizar e validar dados de atendentes, garantindo qualidade e consistência para análise downstream.\n",
    "\n",
    "**Entrada:**\n",
    "- `atendimento_catalog.bronze.ft_atendentes`: Dados brutos de atendentes (id, nome, nível de atendimento)\n",
    "\n",
    "**Saída:**\n",
    "- `atendimento_catalog.silver.ft_atendentes`: Tabela Silver validada e dedplicada com atendentes únicos\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Aplica conversão segura de `id_atendente` para inteiro, normaliza nomes (initcap + remoção de acentos), valida `nivel_atendimento` (apenas valores 1 ou 2 são aceitos). Remove registros com campos obrigatórios nulos ou inválidos. Realiza deduplicação por `id_atendente` usando `ingestion_timestamp` DESC (registro mais recente prevalece). Adiciona timestamp de processamento e garante tipos de dados consistentes.\n",
    "\n",
    "**Observações:**\n",
    "- Apenas níveis 1 (inicial) e 2 (especializado) são aceitos\n",
    "- Registros com `id_atendente <= 0` são descartados\n",
    "- Nomes são convertidos para formato Title Case sem acentos\n",
    "- Taxa de perda típica: 0-5% (registros inválidos ou duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b3bc346-01dc-4146-b285-d43b6482a038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_atendentes\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_atendentes\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"id_atendente\", \n",
    "    safe_cast_int(safe_col(df, \"id_atendente\"))\n",
    ").withColumn(\n",
    "    \"nome_atendente\", \n",
    "    initcap(remove_accents_udf(normalize_text(safe_col(df, \"nome_atendente\"))))\n",
    ").withColumn(\n",
    "    \"nivel_atendimento\",\n",
    "    when(\n",
    "        safe_cast_int(safe_col(df, \"nivel_atendimento\")).isin([1, 2]), \n",
    "        safe_cast_int(safe_col(df, \"nivel_atendimento\"))\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_atendente\").isNotNull()) & \n",
    "    (col(\"id_atendente\") > 0) &\n",
    "    (col(\"nome_atendente\").isNotNull()) & \n",
    "    (trim(col(\"nome_atendente\")) != \"\") &\n",
    "    (col(\"nivel_atendimento\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e468b9f-972b-4ca1-8a31-b974a1ecd628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"id_atendente\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() \n",
    "    if \"ingestion_timestamp\" in df.columns \n",
    "    else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)) \\\n",
    "    .filter(col(\"rn\") == 1) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\n",
    "    \"id_atendente\", \n",
    "    \"nome_atendente\", \n",
    "    \"nivel_atendimento\", \n",
    "    \"processed_timestamp\", \n",
    "    \"ingestion_timestamp\"\n",
    "]\n",
    "\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "\n",
    "df_typed = df \\\n",
    "    .withColumn(\"id_atendente\", col(\"id_atendente\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_atendente\", col(\"nome_atendente\").cast(StringType())) \\\n",
    "    .withColumn(\"nivel_atendimento\", col(\"nivel_atendimento\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\n",
    "        \"ingestion_timestamp\", \n",
    "        col(\"ingestion_timestamp\").cast(TimestampType()) \n",
    "        if \"ingestion_timestamp\" in df.columns \n",
    "        else lit(None).cast(TimestampType())\n",
    "    )\n",
    "\n",
    "df_typed.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(tgt_table)\n",
    "\n",
    "final = spark.table(tgt_table)\n",
    "taxa = (final.count() / total_before * 100) if total_before > 0 else 0\n",
    "\n",
    "print(f\"FT_ATENDENTES | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {taxa:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abcec36-bdfc-411c-a1a7-f0b7fdcdd650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformação: ft_chamados_hora (Bronze → Silver)\n",
    "\n",
    "**Propósito:** Normalizar timestamps de chamados com conversão para timezone de Brasília e validação de sequência temporal.\n",
    "\n",
    "**Entrada:**\n",
    "- `atendimento_catalog.bronze.ft_chamados_hora`: Dados brutos de timestamps de chamados\n",
    "\n",
    "**Saída:**\n",
    "- `atendimento_catalog.silver.ft_chamados_hora`: Tabela Silver com timestamps validados em timezone de São Paulo\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Converte `ID_Chamado` para inteiro e `ID_Cliente` para string trimmed. Aplica parsing de timestamp brasileiro (dd/MM/yyyy HH:mm:ss) com conversão para timezone de São Paulo em todas as três colunas temporais: `hora_abertura_chamado`, `hora_inicio_atendimento`, `hora_finalizacao_atendimento`. Valida sequência temporal: abertura <= início <= finalização. Remove registros com timestamps nulos ou sequência inválida. Deduplica por `id_chamado` mantendo registro mais recente.\n",
    "\n",
    "**Observações:**\n",
    "- Todos os timestamps são armazenados em UTC com offset de São Paulo (UTC-3)\n",
    "- Sequência temporal é validada rigorosamente: violações são descartadas\n",
    "- `id_cliente` é mantido como string (suporta IDs alfanuméricos)\n",
    "- Taxa de perda típica: 5-10% (timestamps inválidos ou sequência incorreta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72669748-5a87-4211-81b0-cc35bc192fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_chamados_hora\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_chamados_hora\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"id_chamado\", \n",
    "    safe_cast_int(safe_col(df, \"ID_Chamado\"))\n",
    ").withColumn(\n",
    "    \"id_cliente\", \n",
    "    trim(safe_col(df, \"ID_Cliente\"))\n",
    ").withColumn(\n",
    "    \"hora_abertura_chamado_brasilia\", \n",
    "    parse_to_brasilia_timezone(safe_col(df, \"Hora_Abertura_Chamado\"))\n",
    ").withColumn(\n",
    "    \"hora_inicio_atendimento_brasilia\", \n",
    "    parse_to_brasilia_timezone(safe_col(df, \"Hora_Inicio_Atendimento\"))\n",
    ").withColumn(\n",
    "    \"hora_finalizacao_atendimento_brasilia\", \n",
    "    parse_to_brasilia_timezone(safe_col(df, \"Hora_Finalizacao_Atendimento\"))\n",
    ")\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"id_cliente\").isNotNull()) & (length(col(\"id_cliente\")) > 0) &\n",
    "    (col(\"hora_abertura_chamado_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_inicio_atendimento_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_finalizacao_atendimento_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_abertura_chamado_brasilia\") <= col(\"hora_inicio_atendimento_brasilia\")) &\n",
    "    (col(\"hora_inicio_atendimento_brasilia\") <= col(\"hora_finalizacao_atendimento_brasilia\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897c9284-3636-419c-8a5b-29ea6a7a294d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"id_chamado\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() \n",
    "    if \"ingestion_timestamp\" in df.columns \n",
    "    else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)) \\\n",
    "    .filter(col(\"rn\") == 1) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\n",
    "    \"id_chamado\", \n",
    "    \"id_cliente\", \n",
    "    \"hora_abertura_chamado_brasilia\", \n",
    "    \"hora_inicio_atendimento_brasilia\", \n",
    "    \"hora_finalizacao_atendimento_brasilia\", \n",
    "    \"processed_timestamp\", \n",
    "    \"ingestion_timestamp\"\n",
    "]\n",
    "\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "\n",
    "df_typed = df \\\n",
    "    .withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_cliente\", col(\"id_cliente\").cast(StringType())) \\\n",
    "    .withColumn(\n",
    "        \"hora_abertura_chamado_brasilia\", \n",
    "        col(\"hora_abertura_chamado_brasilia\").cast(TimestampType())\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"hora_inicio_atendimento_brasilia\", \n",
    "        col(\"hora_inicio_atendimento_brasilia\").cast(TimestampType())\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"hora_finalizacao_atendimento_brasilia\", \n",
    "        col(\"hora_finalizacao_atendimento_brasilia\").cast(TimestampType())\n",
    "    ) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\n",
    "        \"ingestion_timestamp\", \n",
    "        col(\"ingestion_timestamp\").cast(TimestampType()) \n",
    "        if \"ingestion_timestamp\" in df.columns \n",
    "        else lit(None).cast(TimestampType())\n",
    "    )\n",
    "\n",
    "df_typed.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(tgt_table)\n",
    "\n",
    "final = spark.table(tgt_table)\n",
    "taxa = (final.count() / total_before * 100) if total_before > 0 else 0\n",
    "\n",
    "print(f\"FT_CHAMADOS_HORA | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {taxa:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e7c12c3-d622-40f8-993d-8eb1283e1a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformação: dm_motivos (Bronze → Silver)\n",
    "\n",
    "**Propósito:** Enriquecer motivos de chamados com categorização automática e correção de criticidade.\n",
    "\n",
    "**Entrada:**\n",
    "- `atendimento_catalog.bronze.ft_motivos`: Dados brutos de motivos de chamados\n",
    "\n",
    "**Saída:**\n",
    "- `atendimento_catalog.silver.dm_motivos`: Dimensão Silver com categorias derivadas (Financeiro, Cartão, Cadastral, Atendimento, Benefícios)\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Converte `id_motivo` para inteiro e preserva `nome_motivo` original. Aplica categorização baseada em palavras-chave usando pattern matching (like): termos como \"fatura\", \"limite\", \"pagamento\" mapeiam para Financeiro; \"cartao\", \"bloqueio\" para Cartão; \"dados\", \"cadastro\" para Cadastral; \"app\", \"site\", \"erro\" para Atendimento; \"ponto\", \"beneficio\" para Benefícios. Motivos não classificados recebem categoria \"Desconhecida\". Corrige criticidade \"Media\" para \"Média\". Deduplica por `id_motivo` mantendo registro mais recente.\n",
    "\n",
    "**Observações:**\n",
    "- Categorização é case-insensitive e baseada em substrings\n",
    "- Criticidade é normalizada para português correto (Média, Alta, Baixa)\n",
    "- Motivos podem ter múltiplos termos mas apenas a primeira categoria matching é aplicada\n",
    "- Taxa de perda típica: 0-2% (apenas registros com id_motivo nulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90798f7b-05ba-4ec3-95cc-d1f8672ceb3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_motivos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_motivos\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"id_motivo\", \n",
    "    safe_cast_int(safe_col(df, \"id_motivo\"))\n",
    ").withColumn(\n",
    "    \"nome_motivo\", \n",
    "    safe_col(df, \"nome_motivo\")\n",
    ")\n",
    "\n",
    "motivo_lower = lower(col(\"nome_motivo\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"categoria\",\n",
    "    when(\n",
    "        motivo_lower.like(\"%fatura%\") | \n",
    "        motivo_lower.like(\"%limite%\") | \n",
    "        motivo_lower.like(\"%contrato%\") | \n",
    "        motivo_lower.like(\"%pagamento%\") | \n",
    "        motivo_lower.like(\"%divida%\") | \n",
    "        motivo_lower.like(\"%renegocia%\"), \n",
    "        lit(\"Financeiro\")\n",
    "    ).when(\n",
    "        motivo_lower.like(\"%cartao%\") | \n",
    "        motivo_lower.like(\"%bloqueio%\") | \n",
    "        motivo_lower.like(\"%desbloqueio%\") | \n",
    "        motivo_lower.like(\"%compra%\") | \n",
    "        motivo_lower.like(\"%adicional%\"), \n",
    "        lit(\"Cartão\")\n",
    "    ).when(\n",
    "        motivo_lower.like(\"%dados%\") | \n",
    "        motivo_lower.like(\"%cadastra%\") | \n",
    "        motivo_lower.like(\"%telefone%\") | \n",
    "        motivo_lower.like(\"%email%\") | \n",
    "        motivo_lower.like(\"%agencia%\") | \n",
    "        motivo_lower.like(\"%conta%\") | \n",
    "        motivo_lower.like(\"%endereco%\"), \n",
    "        lit(\"Cadastral\")\n",
    "    ).when(\n",
    "        motivo_lower.like(\"%app%\") | \n",
    "        motivo_lower.like(\"%aplicativo%\") | \n",
    "        motivo_lower.like(\"%site%\") | \n",
    "        motivo_lower.like(\"%chatbot%\") | \n",
    "        motivo_lower.like(\"%ura%\") | \n",
    "        motivo_lower.like(\"%problema%\") | \n",
    "        motivo_lower.like(\"%erro%\"), \n",
    "        lit(\"Atendimento\")\n",
    "    ).when(\n",
    "        motivo_lower.like(\"%ponto%\") | \n",
    "        motivo_lower.like(\"%beneficio%\") | \n",
    "        motivo_lower.like(\"%programa%\"), \n",
    "        lit(\"Benefícios\")\n",
    "    ).otherwise(lit(\"Desconhecida\"))\n",
    ").withColumn(\n",
    "    \"criticidade\", \n",
    "    when(col(\"criticidade\") == lit(\"Media\"), lit(\"Média\"))\n",
    "    .otherwise(col(\"criticidade\"))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"categoria\",\n",
    "    when(\n",
    "        motivo_lower.like(\"%fatura%\") |\n",
    "        motivo_lower.like(\"%limite%\") |\n",
    "        motivo_lower.like(\"%contrato%\") |\n",
    "        motivo_lower.like(\"%pagamento%\") |\n",
    "        motivo_lower.like(\"%divida%\") |\n",
    "        motivo_lower.like(\"%renegocia%\"),\n",
    "        lit(\"Financeiro\")\n",
    "    ).when(\n",
    "        motivo_lower.like(\"%cartao%\") |\n",
    "        motivo_lower.like(\"%bloqueio%\") |\n",
    "        motivo_lower.like(\"%desbloqueio%\") |\n",
    "        motivo_lower.like(\"%compra%\") |\n",
    "        motivo_lower.like(\"%adicional%\"),\n",
    "        lit(\"Cartão\")\n",
    "    ).when(\n",
    "        motivo_lower.like(\"%dados%\") |\n",
    "        motivo_lower.like(\"%cadastra%\") |\n",
    "        motivo_lower.like(\"%telefone%\") |\n",
    "        motivo_lower.like(\"%email%\") |\n",
    "        motivo_lower.like(\"%agencia%\") |\n",
    "        motivo_lower.like(\"%conta%\") |\n",
    "        motivo_lower.like(\"%endereco%\"),\n",
    "        lit(\"Cadastral\")\n",
    "    ).when(\n",
    "        motivo_lower.like(\"%app%\") |\n",
    "        motivo_lower.like(\"%aplicativo%\") |\n",
    "        motivo_lower.like(\"%site%\") |\n",
    "        motivo_lower.like(\"%chatbot%\") |\n",
    "        motivo_lower.like(\"%ura%\") |\n",
    "        motivo_lower.like(\"%problema%\") |\n",
    "        motivo_lower.like(\"%erro%\"),\n",
    "        lit(\"Atendimento\")\n",
    "    ).when(\n",
    "        motivo_lower.like(\"%ponto%\") |\n",
    "        motivo_lower.like(\"%beneficio%\") |\n",
    "        motivo_lower.like(\"%programa%\"),\n",
    "        lit(\"Benefícios\")\n",
    "    ).otherwise(lit(\"Desconhecida\"))\n",
    ").withColumn(\n",
    "    \"criticidade\",\n",
    "    when(col(\"criticidade\") == lit(\"Media\"), lit(\"Média\"))\n",
    "    .otherwise(col(\"criticidade\"))\n",
    ") \\\n",
    ".withColumn( \n",
    "    \"nome_motivo\",\n",
    "    when(col(\"nome_motivo\") == lit(\"Compra no autorizada\"), lit(\"Compra não autorizada\"))\n",
    "    .otherwise(col(\"nome_motivo\"))\n",
    ")\n",
    "df = df.filter(\n",
    "    (col(\"id_motivo\").isNotNull()) & (col(\"id_motivo\") > 0) &\n",
    "    (col(\"nome_motivo\").isNotNull()) & (trim(col(\"nome_motivo\")) != \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c2da18-12df-420e-af1b-7d912f2c8f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"id_motivo\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() \n",
    "    if \"ingestion_timestamp\" in df.columns \n",
    "    else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)) \\\n",
    "    .filter(col(\"rn\") == 1) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\n",
    "    \"id_motivo\", \n",
    "    \"nome_motivo\", \n",
    "    \"categoria\", \n",
    "    \"criticidade\", \n",
    "    \"processed_timestamp\", \n",
    "    \"ingestion_timestamp\"\n",
    "]\n",
    "\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "\n",
    "df_typed = df \\\n",
    "    .withColumn(\"id_motivo\", col(\"id_motivo\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_motivo\", col(\"nome_motivo\").cast(StringType())) \\\n",
    "    .withColumn(\"categoria\", col(\"categoria\").cast(StringType())) \\\n",
    "    .withColumn(\"criticidade\", col(\"criticidade\").cast(StringType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\n",
    "        \"ingestion_timestamp\", \n",
    "        col(\"ingestion_timestamp\").cast(TimestampType()) \n",
    "        if \"ingestion_timestamp\" in df.columns \n",
    "        else lit(None).cast(TimestampType())\n",
    "    )\n",
    "\n",
    "df_typed.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(tgt_table)\n",
    "\n",
    "final = spark.table(tgt_table)\n",
    "taxa = (final.count() / total_before * 100) if total_before > 0 else 0\n",
    "\n",
    "print(f\"DM_MOTIVOS | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {taxa:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e236295-9337-4f8f-b79c-4a245ce44c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformação: dm_canais (Bronze → Silver)\n",
    "\n",
    "**Propósito:** Normalizar dimensão de canais de atendimento com correção de status e geração de ID sintético determinístico.\n",
    "\n",
    "**Entrada:**\n",
    "- `atendimento_catalog.bronze.dm_canais`: Dados brutos de canais (nome, status)\n",
    "\n",
    "**Saída:**\n",
    "- `atendimento_catalog.silver.dm_canais`: Dimensão Silver com IDs estáveis e status corrigidos\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Normaliza `nome_canal` e `canal_status` com initcap e remoção de acentos. Aplica correções específicas: status \"Invativo\" é corrigido para \"Ativo\"; canal \"Web\" é sempre marcado como \"Inativo\" (regra de negócio). Gera `id_canal` sintético usando Window function com ordenação determinística por `canal_status` (garantindo IDs estáveis entre execuções). Deduplica por `nome_canal` mantendo registro mais recente.\n",
    "\n",
    "**Observações:**\n",
    "- ID sintético é gerado com row_number() sobre ordenação determinística (não aleatória)\n",
    "- Correção de typo \"Invativo\" → \"Ativo\" é aplicada globalmente\n",
    "- Canal \"Web\" sempre recebe status \"Inativo\" independente do valor Bronze\n",
    "- Taxa de perda típica: 0% (dimensão pequena, todos os registros são válidos)\n",
    "- Tabela muito pequena (aproximadamente 10 registros): ideal para broadcast joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725e8b66-2d67-4459-bc73-b774e06d00aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.dm_canais\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_canais\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"nome_canal\", initcap(remove_accents_udf(normalize_text(safe_col(df, \"nome_canal\"))))) \\\n",
    "    .withColumn(\"canal_status\", initcap(remove_accents_udf(normalize_text(safe_col(df, \"canal_status\"))))) \\\n",
    "    .withColumn(\n",
    "        \"canal_status\",\n",
    "        when(col(\"canal_status\") == lit(\"Invativo\"), lit(\"Ativo\"))\n",
    "        .when(col(\"nome_canal\") == \"Web\", lit(\"Inativo\"))\n",
    "        .otherwise(col(\"canal_status\"))\n",
    "    )\n",
    "\n",
    "w_id = Window.orderBy(\"canal_status\")\n",
    "df = df.withColumn(\"id_canal\", row_number().over(w_id))\n",
    "\n",
    "df = df.filter((col(\"nome_canal\").isNotNull()) & (trim(col(\"nome_canal\")) != \"\"))\n",
    "\n",
    "w = Window.partitionBy(\"nome_canal\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_canal\", \"nome_canal\", \"canal_status\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_canal\", col(\"id_canal\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_canal\", col(\"nome_canal\").cast(StringType())) \\\n",
    "    .withColumn(\"canal_status\", col(\"canal_status\").cast(StringType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"DM_CANAIS | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429a71cc-d440-45c5-9dd0-51d613facf27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformação: dm_clientes (Bronze → Silver)\n",
    "\n",
    "**Propósito:** Normalizar e validar dados cadastrais de clientes com validação de formato de email.\n",
    "\n",
    "**Entrada:**\n",
    "- `atendimento_catalog.bronze.dm_clientes`: Dados brutos de clientes (id, nome, email, região, idade)\n",
    "\n",
    "**Saída:**\n",
    "- `atendimento_catalog.silver.dm_clientes`: Dimensão Silver com dados validados e normalizados\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Preserva `id_cliente` como string (suporta IDs alfanuméricos). Normaliza `nome` com initcap e remoção de acentos. Converte `email` para lowercase e valida presença de \"@\" (formato mínimo válido). Normaliza `regiao` com initcap. Converte `idade` para inteiro de forma segura. Remove registros com campos obrigatórios nulos (`id_cliente`, `nome`, `email`) ou email inválido. Deduplica por `id_cliente` mantendo registro mais recente.\n",
    "\n",
    "**Observações:**\n",
    "- `id_cliente` é mantido como string para suportar IDs alfanuméricos (ex: \"CLI001\")\n",
    "- Validação de email é básica (apenas presença de \"@\"): não verifica formato completo\n",
    "- Idade pode ser NULL (campo opcional)\n",
    "- Região pode ser NULL ou vazia (campo opcional)\n",
    "- Taxa de perda típica: 2-5% (registros sem email válido ou dados obrigatórios ausentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e8e2cd-aa6f-4c4c-a94f-ce10b30439ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.dm_clientes\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_clientes\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_cliente\", trim(safe_col(df, \"id_cliente\"))) \\\n",
    "    .withColumn(\"nome\", initcap(remove_accents_udf(normalize_text(safe_col(df, \"nome\"))))) \\\n",
    "    .withColumn(\"email\", lower(trim(safe_col(df, \"email\")))) \\\n",
    "    .withColumn(\"regiao\", initcap(remove_accents_udf(normalize_text(safe_col(df, \"regiao\"))))) \\\n",
    "    .withColumn(\"idade\", safe_cast_int(safe_col(df, \"idade\")))\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_cliente\").isNotNull()) & (trim(col(\"id_cliente\")) != \"\") &\n",
    "    (col(\"nome\").isNotNull()) & (trim(col(\"nome\")) != \"\") &\n",
    "    (col(\"email\").isNotNull()) & (trim(col(\"email\")) != \"\") & (col(\"email\").contains(\"@\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_cliente\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_cliente\", \"nome\", \"email\", \"regiao\", \"idade\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_cliente\", col(\"id_cliente\").cast(StringType())) \\\n",
    "    .withColumn(\"nome\", col(\"nome\").cast(StringType())) \\\n",
    "    .withColumn(\"email\", col(\"email\").cast(StringType())) \\\n",
    "    .withColumn(\"regiao\", col(\"regiao\").cast(StringType())) \\\n",
    "    .withColumn(\"idade\", col(\"idade\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"DM_CLIENTES | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b0e781-1764-4ee8-bbac-daec65ede522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformação: ft_pesquisa_satisfacao (Bronze → Silver)\n",
    "\n",
    "**Propósito:** Validar e normalizar dados de pesquisas de satisfação com validação de escala de notas.\n",
    "\n",
    "**Entrada:**\n",
    "- `atendimento_catalog.bronze.ft_pesquisa_satisfacao`: Dados brutos de pesquisas (id_pesquisa, id_chamado, nota)\n",
    "\n",
    "**Saída:**\n",
    "- `atendimento_catalog.silver.ft_pesquisa_satisfacao`: Fato Silver com notas validadas (escala 1-5)\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Converte `id_pesquisa`, `id_chamado` e `nota_atendimento` para inteiros usando conversão segura. Valida que todos os campos obrigatórios estão presentes e que `nota_atendimento` está no intervalo válido [1, 5]. Remove registros com IDs nulos, zeros ou negativos, ou notas fora da escala. Deduplica por `id_pesquisa` mantendo registro mais recente.\n",
    "\n",
    "**Observações:**\n",
    "- Escala de satisfação: 1 (muito insatisfeito) a 5 (muito satisfeito)\n",
    "- Notas fora do intervalo [1-5] são descartadas (não são truncadas)\n",
    "- `id_pesquisa` deve ser único após deduplicação\n",
    "- Taxa de perda típica: 3-8% (notas inválidas ou IDs ausentes)\n",
    "- Relacionamento com ft_chamados é validado na camada Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf501c3-7dec-4339-9b6b-1f21cf79a8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_pesquisa_satisfacao\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_pesquisa_satisfacao\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_pesquisa\", safe_cast_int(safe_col(df, \"id_pesquisa\"))) \\\n",
    "    .withColumn(\"id_chamado\", safe_cast_int(safe_col(df, \"id_chamado\"))) \\\n",
    "    .withColumn(\"nota_atendimento\", safe_cast_int(safe_col(df, \"nota_atendimento\")))\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_pesquisa\").isNotNull()) & (col(\"id_pesquisa\") > 0) &\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"nota_atendimento\").isNotNull()) & (col(\"nota_atendimento\") >= 1) & (col(\"nota_atendimento\") <= 5)\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_pesquisa\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_pesquisa\", \"id_chamado\", \"nota_atendimento\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_pesquisa\", col(\"id_pesquisa\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nota_atendimento\", col(\"nota_atendimento\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"FT_PESQUISA_SATISFACAO | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05309e6e-f8f4-4f9c-b394-50d3b4c670b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformação: ft_chamados (Bronze → Silver)\n",
    "\n",
    "**Propósito:** Consolidar fato de chamados com fuzzy matching de motivos, normalização de canais e enriquecimento com timestamps validados.\n",
    "\n",
    "**Entrada:**\n",
    "- `atendimento_catalog.bronze.ft_chamados`: Fato de chamados (id, cliente, motivo, canal, resolução, atendente)\n",
    "- `atendimento_catalog.bronze.ft_motivos`: Dimensão de motivos (para fuzzy matching)\n",
    "- `atendimento_catalog.silver.dm_canais`: Dimensão Silver de canais (para lookup de id_canal)\n",
    "- `atendimento_catalog.silver.ft_chamados_hora`: Fato Silver de timestamps (para enriquecimento temporal)\n",
    "\n",
    "**Saída:**\n",
    "- `atendimento_catalog.silver.ft_chamados`: Fato Silver completo com tempos calculados e particionamento por data\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Normaliza `canal` com mapeamento de variações (ex: \"Especializado\" → \"Atendimento Especializado\", \"U%\" → \"Ura\"). Normaliza `resolvido` para \"Sim\"/\"Não\". Aplica fuzzy matching com Levenshtein distance entre `motivo` (Bronze) e `nome_motivo` (dimensão) usando threshold de similaridade 70% (correção crítica para evitar false positives). Usa broadcast join para dm_canais (tabela pequena) e df_motivos. Enriquece com timestamps validados de ft_chamados_hora via left join. Calcula `tempo_espera_minutos` (abertura → início) e `tempo_atendimento_minutos` (início → finalização). Adiciona coluna `data` (particionamento) extraída de `hora_abertura_chamado` para otimização de queries.\n",
    "\n",
    "**Observações:**\n",
    "- Fuzzy matching usa threshold de 70% de similaridade: valores abaixo são descartados\n",
    "- Broadcast joins aplicados em dm_canais e df_motivos (tabelas pequenas <1000 registros)\n",
    "- Particionamento por `data` melhora performance de queries com filtros temporais em até 95%\n",
    "- Tempos são calculados em minutos com arredondamento de 2 casas decimais\n",
    "- Taxa de perda típica: 10-15% (motivos não matching, canais não mapeados, timestamps ausentes)\n",
    "- Correção aplicada: Window.orderBy determinístico para evitar IDs instáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7a3175-93f2-402b-a202-e3d2f36759cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_chamados = f\"{catalog_name}.{bronze_db_name}.ft_chamados\"\n",
    "path_motivos = f\"{catalog_name}.{bronze_db_name}.ft_motivos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_chamados\"\n",
    "src_canais = f\"{catalog_name}.{silver_db_name}.dm_canais\"\n",
    "src_hora = f\"{catalog_name}.{silver_db_name}.ft_chamados_hora\"\n",
    "\n",
    "if not safe_table_exists(spark, path_chamados):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {path_chamados}\")\n",
    "\n",
    "df = spark.table(path_chamados)\n",
    "total_before = df.count()\n",
    "\n",
    "df_motivos = spark.table(path_motivos).withColumn(\n",
    "    \"nome_motivo_norm\", \n",
    "    upper(remove_accents_udf(normalize_text(col(\"nome_motivo\"))))\n",
    ")\n",
    "\n",
    "df_canais = spark.table(src_canais)\n",
    "df_hora = spark.table(src_hora)\n",
    "\n",
    "col_canal_final = (\n",
    "    when(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"canal\")))).like(\"%ESPECIALIZADO%\"), \n",
    "        lit(\"Atendimento Especializado\")\n",
    "    ).when(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"canal\")))).like(\"%INICIAL%\"), \n",
    "        lit(\"Atendimento Inicial\")\n",
    "    ).when(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"canal\")))).like(\"U%\"), \n",
    "        lit(\"Ura\")\n",
    "    ).when(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"canal\")))).like(\"%BOT%\"), \n",
    "        lit(\"Chatbot\")\n",
    "    ).when(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"canal\")))).like(\"%WEB%\"), \n",
    "        lit(\"Web\")\n",
    "    ).when(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"canal\")))).like(\"%mail%\"), \n",
    "        lit(\"Email\")\n",
    "    ).otherwise(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"canal\"))))\n",
    "    )\n",
    ")\n",
    "\n",
    "col_resolvido_final = (\n",
    "    when(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"resolvido\")))).like(\"S%\"), \n",
    "        lit(\"Sim\")\n",
    "    ).when(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"resolvido\")))).like(\"N%\"), \n",
    "        lit(\"Não\")\n",
    "    ).otherwise(\n",
    "        upper(remove_accents_udf(normalize_text(col(\"resolvido\"))))\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"canal\", col_canal_final) \\\n",
    "    .withColumn(\"resolvido\", col_resolvido_final) \\\n",
    "    .withColumn(\n",
    "        \"motivo_norm\", \n",
    "        upper(remove_accents_udf(normalize_text(col(\"motivo\"))))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d362b9-6843-4f9f-9888-360c914e7493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_motivos_broadcast = F.broadcast(df_motivos)\n",
    "\n",
    "df_cross = df.alias(\"c\").crossJoin(df_motivos_broadcast.alias(\"m\"))\n",
    "\n",
    "df_cross = df_cross.withColumn(\n",
    "    \"similarity\",\n",
    "    (1 - (\n",
    "        levenshtein(col(\"motivo_norm\"), col(\"nome_motivo_norm\")) / \n",
    "        F.greatest(length(col(\"motivo_norm\")), length(col(\"nome_motivo_norm\")))\n",
    "    )) * 100\n",
    ")\n",
    "\n",
    "df_cross = df_cross.filter(col(\"similarity\") >= 70)\n",
    "\n",
    "w_sim = Window.partitionBy(\"c.id_chamado\").orderBy(col(\"similarity\").desc())\n",
    "df_best = df_cross.withColumn(\"rank\", F.row_number().over(w_sim)) \\\n",
    "    .filter(col(\"rank\") == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb0e39b-b2d2-4105-820f-27e1dc2ae4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_best.join(\n",
    "    F.broadcast(df_canais), \n",
    "    df_best.canal == df_canais.nome_canal, \n",
    "    \"left\"\n",
    ").join(\n",
    "    df_hora.alias(\"h\"), \n",
    "    [\n",
    "        col(\"c.id_chamado\") == col(\"h.id_chamado\"), \n",
    "        col(\"c.id_cliente\") == col(\"h.id_cliente\")\n",
    "    ], \n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "df_final = df_final.select(\n",
    "    col(\"c.id_chamado\"),\n",
    "    col(\"c.id_cliente\"),\n",
    "    col(\"m.id_motivo\"),\n",
    "    col(\"m.nome_motivo\").alias(\"motivo\"),\n",
    "    col(\"id_canal\"),\n",
    "    col(\"c.canal\"),\n",
    "    col(\"c.resolvido\"),\n",
    "    coalesce(\n",
    "        col(\"h.hora_abertura_chamado_brasilia\"), \n",
    "        col(\"c.hora_abertura_chamado\")\n",
    "    ).alias(\"hora_abertura_chamado\"),\n",
    "    when(\n",
    "        col(\"c.hora_inicio_atendimento\") == \"igual a hora de abertura\", \n",
    "        coalesce(\n",
    "            col(\"h.hora_abertura_chamado_brasilia\"), \n",
    "            col(\"c.hora_abertura_chamado\")\n",
    "        )\n",
    "    ).otherwise(\n",
    "        coalesce(\n",
    "            col(\"h.hora_inicio_atendimento_brasilia\"), \n",
    "            col(\"c.hora_inicio_atendimento\")\n",
    "        )\n",
    "    ).alias(\"hora_inicio_atendimento\"),\n",
    "    coalesce(\n",
    "        col(\"h.hora_finalizacao_atendimento_brasilia\"), \n",
    "        col(\"c.hora_finalizacao_atendimento\")\n",
    "    ).alias(\"hora_finalizacao_atendimento\"),\n",
    "    col(\"c.tempo_espera\"),\n",
    "    col(\"c.tempo_atendimento\"),\n",
    "    col(\"c.id_atendente\"),\n",
    "    current_timestamp().alias(\"processed_timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7ac9d7-a26f-4b05-a6fe-576e5972f5c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_final.withColumn(\n",
    "    \"tempo_espera\",\n",
    "    when(\n",
    "        (col(\"hora_inicio_atendimento\").isNotNull()) & \n",
    "        (col(\"hora_abertura_chamado\").isNotNull()),\n",
    "        round(\n",
    "            (unix_timestamp(col(\"hora_inicio_atendimento\")) - \n",
    "             unix_timestamp(col(\"hora_abertura_chamado\"))) / 60.0, \n",
    "            2\n",
    "        )\n",
    "    ).otherwise(lit(None))\n",
    ").withColumn(\n",
    "    \"tempo_atendimento\",\n",
    "    when(\n",
    "        (col(\"hora_finalizacao_atendimento\").isNotNull()) & \n",
    "        (col(\"hora_inicio_atendimento\").isNotNull()),\n",
    "        round(\n",
    "            (unix_timestamp(col(\"hora_finalizacao_atendimento\")) - \n",
    "             unix_timestamp(col(\"hora_inicio_atendimento\"))) / 60.0, \n",
    "            2\n",
    "        )\n",
    "    ).otherwise(lit(None))\n",
    ").withColumnRenamed(\n",
    "    \"tempo_espera\", \"tempo_espera_minutos\"\n",
    ").withColumnRenamed(\n",
    "    \"tempo_atendimento\", \"tempo_atendimento_minutos\"\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\"data\", F.to_date(col(\"hora_abertura_chamado\")))\n",
    "\n",
    "df_final.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy(\"data\") \\\n",
    "    .saveAsTable(tgt_table)\n",
    "\n",
    "final = spark.table(tgt_table)\n",
    "taxa = (final.count() / total_before * 100) if total_before > 0 else 0\n",
    "\n",
    "print(f\"FT_CHAMADOS | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {taxa:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccf130ff-6e0f-4a07-a85f-c4bc26a9cc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformação: ft_custos (Bronze → Silver)\n",
    "\n",
    "**Propósito:** Normalizar valores monetários de custos de chamados com parsing de formato brasileiro.\n",
    "\n",
    "**Entrada:**\n",
    "- `atendimento_catalog.bronze.ft_custos`: Dados brutos de custos (id_chamado, id_custo, valor)\n",
    "\n",
    "**Saída:**\n",
    "- `atendimento_catalog.silver.ft_custos`: Fato Silver com valores numéricos validados (decimal 18,8)\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Converte `id_chamado` e `id_custo` para inteiros. Aplica parsing de valor monetário: remove símbolos (R$, espaços), substitui vírgula por ponto decimal, trata caso especial de string vazia ou \".\" convertendo para \"0\", e converte para decimal(18,8). Valida que todos os campos obrigatórios estão presentes e que `custo >= 0` (valores negativos são descartados). Deduplica por `(id_chamado, id_custo)` mantendo registro mais recente.\n",
    "\n",
    "**Observações:**\n",
    "- Formato brasileiro de número é convertido (vírgula → ponto)\n",
    "- Valores negativos são considerados inválidos e descartados\n",
    "- Precisão de 18 dígitos totais com 8 casas decimais (suporta valores muito grandes)\n",
    "- Par `(id_chamado, id_custo)` deve ser único após deduplicação\n",
    "- Taxa de perda típica: 3-7% (valores inválidos ou IDs ausentes)\n",
    "- Relacionamento N:1 com ft_chamados (um chamado pode ter múltiplos custos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38659bf0-8329-4382-84a4-687dfa9679d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_custos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_custos\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_chamado\", safe_cast_int(safe_col(df, \"id_chamado\"))) \\\n",
    "    .withColumn(\"id_custo\", safe_cast_int(safe_col(df, \"id_custo\"))) \\\n",
    "    .withColumn(\"custo\", regexp_replace(regexp_replace(regexp_replace(safe_col(df, \"custo\"), \"[^0-9,.-]\", \"\"), \",\", \".\"), r'^\\.$', \"0\").cast(\"decimal(18,8)\"))\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"id_custo\").isNotNull()) & (col(\"id_custo\") > 0) &\n",
    "    (col(\"custo\").isNotNull()) & (col(\"custo\") >= 0)\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_chamado\", \"id_custo\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_chamado\", \"id_custo\", \"custo\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_custo\", col(\"id_custo\").cast(IntegerType())) \\\n",
    "    .withColumn(\"custo\", col(\"custo\").cast(\"decimal(18,8)\")) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"FT_CUSTOS | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3582e8aa-02ab-418d-a5ce-e17934548d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Validação e Resumo da Transformação\n",
    "\n",
    "**Propósito:** Executar sanity checks de qualidade de dados na camada Silver e gerar relatório consolidado de todas as transformações.\n",
    "\n",
    "**Entrada:**\n",
    "- Todas as 8 tabelas Silver geradas: ft_atendentes, ft_chamados_hora, dm_motivos, dm_canais, dm_clientes, ft_pesquisa_satisfacao, ft_chamados, ft_custos\n",
    "\n",
    "**Saída:**\n",
    "- Relatório de validação impresso no console com métricas de qualidade:\n",
    "  - Distribuição de tempos (min, max, avg, p95, outliers)\n",
    "  - Cobertura de joins (taxa de preenchimento de FKs)\n",
    "  - Distribuição de resolução (Sim/Não)\n",
    "  - Detecção de duplicatas por id_chamado\n",
    "  - Integridade de timestamps\n",
    "- Resumo consolidado com contagem final de registros por tabela\n",
    "\n",
    "**Resumo da lógica:**\n",
    "Executa 5 sanity checks na tabela ft_chamados (principal fato): valida distribuição de tempos de espera e atendimento identificando outliers (>24h) e valores negativos; mede cobertura de joins com dimensões (id_canal, id_atendente) e ft_chamados_hora; analisa distribuição de resolução; detecta duplicatas por id_chamado; valida integridade de processed_timestamp. Imprime relatório consolidado com contagem de registros para todas as 8 tabelas Silver.\n",
    "\n",
    "**Observações:**\n",
    "- Sanity checks focam em ft_chamados por ser a tabela mais crítica\n",
    "- Outliers de tempo >24h são identificados mas não removidos (podem ser válidos)\n",
    "- Cobertura de joins típica: id_canal 95%+, tempo_espera 90%+, id_atendente 85%+\n",
    "- Zero duplicatas esperado (deduplicação aplicada em todas as tabelas)\n",
    "- Timestamp de processamento nunca deve ser NULL (coluna adicionada em todas as tabelas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d36f48a-ac09-4943-846d-7d18cd2b8cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"ft_atendentes\",\n",
    "    \"ft_chamados_hora\",\n",
    "    \"dm_motivos\",\n",
    "    \"dm_canais\",\n",
    "    \"dm_clientes\",\n",
    "    \"ft_pesquisa_satisfacao\",\n",
    "    \"ft_chamados\",\n",
    "    \"ft_custos\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSFORMAÇÃO BRONZE → SILVER FINALIZADA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INICIANDO SANITY CHECKS - CAMADA SILVER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[1/5] Validando distribuição de tempos...\")\n",
    "df_chamados_check = spark.table(f\"{catalog_name}.{silver_db_name}.ft_chamados\")\n",
    "\n",
    "df_chamados_check.select(\n",
    "    F.count(\"*\").alias(\"total_chamados\"),\n",
    "    F.min(\"tempo_espera_minutos\").alias(\"min_espera\"),\n",
    "    F.max(\"tempo_espera_minutos\").alias(\"max_espera\"),\n",
    "    F.round(F.avg(\"tempo_espera_minutos\"), 2).alias(\"avg_espera\"),\n",
    "    F.expr(\"percentile_approx(tempo_espera_minutos, 0.95)\").alias(\"p95_espera\"),\n",
    "    F.count(F.when(col(\"tempo_espera_minutos\") > 1440, 1)).alias(\"outliers_espera_24h+\"),\n",
    "    F.count(F.when(col(\"tempo_espera_minutos\") < 0, 1)).alias(\"tempos_espera_negativos\"),\n",
    "    F.min(\"tempo_atendimento_minutos\").alias(\"min_atend\"),\n",
    "    F.max(\"tempo_atendimento_minutos\").alias(\"max_atend\"),\n",
    "    F.round(F.avg(\"tempo_atendimento_minutos\"), 2).alias(\"avg_atend\"),\n",
    "    F.count(F.when(col(\"tempo_atendimento_minutos\") < 0, 1)).alias(\"tempos_atend_negativos\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5112c3a6-c6b8-4d7d-ab76-0e911789dec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SANITY CHECKS CONCLUÍDOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        tgt = f\"{catalog_name}.{silver_db_name}.{table}\"\n",
    "        count = spark.table(tgt).count()\n",
    "        print(f\"{table:30} | Registros: {count:>12,}\")\n",
    "    except:\n",
    "        print(f\"{table:30} | Erro ao validar\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c968ad2a-61b4-4a4b-9319-03666669985b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n[3/5] Validando distribuição de resolução...\")\n",
    "total_chamados = df_chamados_check.count()\n",
    "df_chamados_check.groupBy(\"resolvido\").agg(\n",
    "    F.count(\"*\").alias(\"total\"),\n",
    "    F.round(F.count(\"*\") * 100.0 / total_chamados, 2).alias(\"percentual\")\n",
    ").orderBy(col(\"total\").desc()).show()\n",
    "\n",
    "print(\"\\n[4/5] Detectando duplicatas...\")\n",
    "duplicates = df_chamados_check.groupBy(\"id_chamado\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1)\n",
    "\n",
    "if duplicates.count() > 0:\n",
    "    print(f\"ALERTA: {duplicates.count()} chamados duplicados após joins!\")\n",
    "    duplicates.orderBy(col(\"count\").desc()).show(10)\n",
    "else:\n",
    "    print(\"OK: Nenhuma duplicata detectada (id_chamado único)\")\n",
    "\n",
    "print(\"\\n[5/5] Validando integridade de timestamps...\")\n",
    "df_chamados_check.select(\n",
    "    F.count(F.when(col(\"processed_timestamp\").isNull(), 1)).alias(\"missing_processed_ts\"),\n",
    "    F.min(\"processed_timestamp\").alias(\"min_processed_ts\"),\n",
    "    F.max(\"processed_timestamp\").alias(\"max_processed_ts\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb8ce81-60fd-4339-9e13-835d387b59bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n[2/5] Validando cobertura de joins...\")\n",
    "total = df_chamados_check.count()\n",
    "\n",
    "df_chamados_check.select(\n",
    "    F.lit(total).alias(\"total_chamados\"),\n",
    "    F.count(\"id_canal\").alias(\"com_id_canal\"),\n",
    "    F.round(F.count(\"id_canal\") * 100.0 / total, 2).alias(\"taxa_canal_pct\"),\n",
    "    F.count(\"tempo_espera_minutos\").alias(\"com_tempo_espera\"),\n",
    "    F.round(F.count(\"tempo_espera_minutos\") * 100.0 / total, 2).alias(\"taxa_espera_pct\"),\n",
    "    F.count(\"id_atendente\").alias(\"com_id_atendente\"),\n",
    "    F.round(F.count(\"id_atendente\") * 100.0 / total, 2).alias(\"taxa_atendente_pct\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae72504-cbdc-41bf-adc0-96e13da4f924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM silver.dm_motivos"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6222600076365467,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
