{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7c7950-7e18-4905-95b4-ba33b8790f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, lower, initcap, current_timestamp,\n",
    "    lit, coalesce, when, regexp_replace, length, row_number, to_timestamp, levenshtein, unix_timestamp, round\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "catalog_name = \"atendimento_catalog\"\n",
    "bronze_db_name = \"bronze\"\n",
    "silver_db_name = \"silver\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {silver_db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9b3a16-03ec-44f0-a072-bfa6659ffbc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"atendimento_catalog\"\n",
    "bronze_db_name = \"bronze\"\n",
    "silver_db_name = \"silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d643b4fa-79c9-45c4-bae5-2c3261ee395f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {silver_db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63af09c6-021b-4f88-9869-b9bda470e3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def safe_table_exists(spark, full_name: str) -> bool:\n",
    "    try:\n",
    "        spark.table(full_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def safe_col(df, name):\n",
    "    return col(name) if name in df.columns else lit(None).cast(StringType())\n",
    "\n",
    "def safe_cast_int(col_expr):\n",
    "    return when(\n",
    "        col_expr.isNotNull() & (trim(col_expr).cast(StringType()) != \"\"),\n",
    "        F.regexp_replace(trim(col_expr).cast(StringType()), r'[^\\d]', '').cast(IntegerType())\n",
    "    ).otherwise(None)\n",
    "\n",
    "def remove_accents_udf(text_col):\n",
    "    return F.translate(\n",
    "        text_col,\n",
    "        \"áàãâäéèêëíìîïóòõôöúùûüçñÁÀÃÂÄÉÈÊËÍÌÎÏÓÒÕÔÖÚÙÛÜÇÑ\",\n",
    "        \"aaaaaeeeeiiiiooooouuuucnAAAAAEEEEIIIIOOOOOUUUUCN\"\n",
    "    )\n",
    "\n",
    "def normalize_text(col_expr):\n",
    "    return trim(regexp_replace(regexp_replace(col_expr, r'\\s+', ' '), r'[^\\x20-\\x7E\\u00C0-\\u00FF]', ''))\n",
    "\n",
    "def parse_to_brasilia_timezone(col_expr):\n",
    "    cleaned = regexp_replace(col_expr, r'[ ]', ' ')\n",
    "    cleaned = regexp_replace(cleaned, r'\\bàs\\b|\\bas\\b|\\bàs\\b', ' ')\n",
    "    cleaned = regexp_replace(cleaned, r'\\s+', ' ')\n",
    "    cleaned = trim(cleaned)\n",
    "    parsed_naive = to_timestamp(cleaned, 'dd/MM/yyyy HH:mm:ss')\n",
    "    parsed_utc = F.to_utc_timestamp(parsed_naive, 'America/Sao_Paulo')\n",
    "    return F.from_utc_timestamp(parsed_utc, 'America/Sao_Paulo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e208aa91-47b2-4168-9a4a-4fe9229b76a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento: ft_atendentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f10fdfd8-6956-4f00-86f7-0431b32bf959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_atendentes\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_atendentes\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df_src = spark.table(src_table)\n",
    "total_before = df_src.count()\n",
    "print(f\"Bronze: {total_before:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db4c04ae-af94-4e61-a334-9859e17e1e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_src \\\n",
    "    .withColumn(\"id_atendente\", safe_cast_int(safe_col(df_src, \"id_atendente\"))) \\\n",
    "    .withColumn(\"nome_atendente\", initcap(remove_accents_udf(normalize_text(safe_col(df_src, \"nome_atendente\"))))) \\\n",
    "    .withColumn(\"nivel_atendimento\", when(safe_cast_int(safe_col(df_src, \"nivel_atendimento\")).isin([1, 2]), safe_cast_int(safe_col(df_src, \"nivel_atendimento\"))).otherwise(None))\n",
    "\n",
    "df_valid = df.filter(\n",
    "    (col(\"id_atendente\").isNotNull()) & (col(\"id_atendente\") > 0) &\n",
    "    (col(\"nome_atendente\").isNotNull()) & (trim(col(\"nome_atendente\")) != \"\") &\n",
    "    (col(\"nivel_atendimento\").isNotNull())\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_atendente\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df_valid.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df_dedup = df_valid.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "df_final = df_dedup.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "cols = [\"id_atendente\", \"nome_atendente\", \"nivel_atendimento\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df_final = df_final.select(*[c for c in cols if c in df_final.columns])\n",
    "\n",
    "df_typed = df_final \\\n",
    "    .withColumn(\"id_atendente\", col(\"id_atendente\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_atendente\", col(\"nome_atendente\").cast(StringType())) \\\n",
    "    .withColumn(\"nivel_atendimento\", col(\"nivel_atendimento\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df_final.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "print(f\"Silver: {df_typed.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33efd9df-df2c-4a34-9e7a-6d0337c9f852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_typed.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "print(f\"Salvo: {tgt_table}\")\n",
    "display(spark.table(tgt_table).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d78179-d069-4ba2-bf10-4c26f56586b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento: ft_chamados_hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52bfadc3-4838-49fe-99a9-4ffa5129150d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_chamados_hora\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_chamados_hora\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df_src = spark.table(src_table)\n",
    "total_before = df_src.count()\n",
    "print(f\"Bronze: {total_before:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3699aba5-79d6-4609-ab19-58769022230f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_src \\\n",
    "    .withColumn(\"id_chamado\", safe_cast_int(safe_col(df_src, \"ID_Chamado\"))) \\\n",
    "    .withColumn(\"id_cliente\", trim(safe_col(df_src, \"ID_Cliente\"))) \\\n",
    "    .withColumn(\"hora_abertura_chamado_brasilia\", parse_to_brasilia_timezone(safe_col(df_src, \"Hora_Abertura_Chamado\"))) \\\n",
    "    .withColumn(\"hora_inicio_atendimento_brasilia\", parse_to_brasilia_timezone(safe_col(df_src, \"Hora_Inicio_Atendimento\"))) \\\n",
    "    .withColumn(\"hora_finalizacao_atendimento_brasilia\", parse_to_brasilia_timezone(safe_col(df_src, \"Hora_Finalizacao_Atendimento\")))\n",
    "\n",
    "df_valid = df.filter(\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"id_cliente\").isNotNull()) & (length(col(\"id_cliente\")) > 0) &\n",
    "    (col(\"hora_abertura_chamado_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_inicio_atendimento_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_finalizacao_atendimento_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_abertura_chamado_brasilia\") <= col(\"hora_inicio_atendimento_brasilia\")) &\n",
    "    (col(\"hora_inicio_atendimento_brasilia\") <= col(\"hora_finalizacao_atendimento_brasilia\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_chamado\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df_valid.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df_dedup = df_valid.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "df_final = df_dedup.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "cols = [\"id_chamado\", \"id_cliente\", \"hora_abertura_chamado_brasilia\", \"hora_inicio_atendimento_brasilia\", \"hora_finalizacao_atendimento_brasilia\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df_final = df_final.select(*[c for c in cols if c in df_final.columns])\n",
    "\n",
    "df_typed = df_final \\\n",
    "    .withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_cliente\", col(\"id_cliente\").cast(StringType())) \\\n",
    "    .withColumn(\"hora_abertura_chamado_brasilia\", col(\"hora_abertura_chamado_brasilia\").cast(TimestampType())) \\\n",
    "    .withColumn(\"hora_inicio_atendimento_brasilia\", col(\"hora_inicio_atendimento_brasilia\").cast(TimestampType())) \\\n",
    "    .withColumn(\"hora_finalizacao_atendimento_brasilia\", col(\"hora_finalizacao_atendimento_brasilia\").cast(TimestampType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df_final.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "print(f\"Silver: {df_typed.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6b85ca-03f5-4647-9877-0e4a73d98252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_typed.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "silver_chamados_hora = spark.table(tgt_table)\n",
    "print(f\"Salvo: {tgt_table}\")\n",
    "display(silver_chamados_hora.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0634a864-65e3-4929-b316-f069d7980cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento: dm_motivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14497b0-8138-494e-b9d6-b9837680789e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_motivos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_motivos\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df_src = spark.table(src_table)\n",
    "total_before = df_src.count()\n",
    "print(f\"Bronze: {total_before:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01238b2-c9dc-4f95-9a9f-d746da6f05b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_src \\\n",
    "    .withColumn(\"id_motivo\", safe_cast_int(safe_col(df_src, \"id_motivo\"))) \\\n",
    "    .withColumn(\"nome_motivo\", safe_col(df_src, \"nome_motivo\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"categoria\",\n",
    "    when((lower(col(\"nome_motivo\")).like(\"%fatura%\")) | (lower(col(\"nome_motivo\")).like(\"%limite%\")) | (lower(col(\"nome_motivo\")).like(\"%contrato%\")) | (lower(col(\"nome_motivo\")).like(\"%pagamento%\")) | (lower(col(\"nome_motivo\")).like(\"%divida%\")) | (lower(col(\"nome_motivo\")).like(\"%renegocia%\")), lit(\"Financeiro\"))\n",
    "    .when((lower(col(\"nome_motivo\")).like(\"%cartao%\")) | (lower(col(\"nome_motivo\")).like(\"%bloqueio%\")) | (lower(col(\"nome_motivo\")).like(\"%desbloqueio%\")) | (lower(col(\"nome_motivo\")).like(\"%compra%\")) | (lower(col(\"nome_motivo\")).like(\"%adicional%\")), lit(\"Cartão\"))\n",
    "    .when((lower(col(\"nome_motivo\")).like(\"%dados%\")) | (lower(col(\"nome_motivo\")).like(\"%cadastra%\")) | (lower(col(\"nome_motivo\")).like(\"%telefone%\")) | (lower(col(\"nome_motivo\")).like(\"%email%\")) | (lower(col(\"nome_motivo\")).like(\"%agencia%\")) | (lower(col(\"nome_motivo\")).like(\"%conta%\")) | (lower(col(\"nome_motivo\")).like(\"%endereco%\")), lit(\"Cadastral\"))\n",
    "    .when((lower(col(\"nome_motivo\")).like(\"%app%\")) | (lower(col(\"nome_motivo\")).like(\"%aplicativo%\")) | (lower(col(\"nome_motivo\")).like(\"%site%\")) | (lower(col(\"nome_motivo\")).like(\"%chatbot%\")) | (lower(col(\"nome_motivo\")).like(\"%ura%\")) | (lower(col(\"nome_motivo\")).like(\"%problema%\")) | (lower(col(\"nome_motivo\")).like(\"%erro%\")), lit(\"Atendimento\"))\n",
    "    .when((lower(col(\"nome_motivo\")).like(\"%ponto%\")) | (lower(col(\"nome_motivo\")).like(\"%beneficio%\")) | (lower(col(\"nome_motivo\")).like(\"%programa%\")), lit(\"Benefícios\"))\n",
    "    .otherwise(lit(\"Desconhecida\"))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"criticidade\", when(initcap(remove_accents_udf(normalize_text(safe_col(df_src, \"criticidade\")))) == lit(\"Media\"), lit(\"Média\")).otherwise(initcap(remove_accents_udf(normalize_text(safe_col(df_src, \"criticidade\"))))))\n",
    "\n",
    "df_valid = df.filter((col(\"id_motivo\").isNotNull()) & (col(\"id_motivo\") > 0) & (col(\"nome_motivo\").isNotNull()) & (trim(col(\"nome_motivo\")) != \"\"))\n",
    "\n",
    "w = Window.partitionBy(\"id_motivo\").orderBy(col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df_valid.columns else lit(datetime.now()))\n",
    "\n",
    "df_dedup = df_valid.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "df_final = df_dedup.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "cols = [\"id_motivo\", \"nome_motivo\", \"categoria\", \"criticidade\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df_final = df_final.select(*[c for c in cols if c in df_final.columns])\n",
    "\n",
    "df_typed = df_final \\\n",
    "    .withColumn(\"id_motivo\", col(\"id_motivo\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_motivo\", col(\"nome_motivo\").cast(StringType())) \\\n",
    "    .withColumn(\"categoria\", col(\"categoria\").cast(StringType())) \\\n",
    "    .withColumn(\"criticidade\", col(\"criticidade\").cast(StringType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df_final.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "print(f\"Silver: {df_typed.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787cf274-8cb1-409b-b712-49ab8d14044d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_typed.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "print(f\"Salvo: {tgt_table}\")\n",
    "display(spark.table(tgt_table).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44044dbe-aa18-49a3-b8ed-7b1b6fc72577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento: dm_canais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c619ff0a-4e43-477e-bf65-2f5beaf08340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.dm_canais\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_canais\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df_src = spark.table(src_table)\n",
    "total_before = df_src.count()\n",
    "print(f\"Bronze: {total_before:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c7767d-5330-42db-8a04-5121a21cff69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_src \\\n",
    "    .withColumn(\"nome_canal\", initcap(remove_accents_udf(normalize_text(safe_col(df_src, \"nome_canal\"))))) \\\n",
    "    .withColumn(\"canal_status\", initcap(remove_accents_udf(normalize_text(safe_col(df_src, \"canal_status\")))))\n",
    "\n",
    "df = df.withColumn(\"canal_status\", when(col(\"canal_status\") == lit(\"Invativo\"), lit(\"Ativo\")).otherwise(col(\"canal_status\")))\n",
    "df = df.withColumn(\"canal_status\", when(col(\"nome_canal\") == \"Web\", lit(\"Inativo\")).otherwise(col(\"canal_status\")))\n",
    "\n",
    "w_id = Window.orderBy(\"canal_status\")\n",
    "df = df.withColumn(\"id_canal\", row_number().over(w_id))\n",
    "\n",
    "df_valid = df.filter((col(\"nome_canal\").isNotNull()) & (trim(col(\"nome_canal\")) != \"\"))\n",
    "\n",
    "w = Window.partitionBy(\"nome_canal\").orderBy(col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df_valid.columns else lit(datetime.now()))\n",
    "\n",
    "df_dedup = df_valid.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "df_final = df_dedup.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "cols = [\"id_canal\", \"nome_canal\", \"canal_status\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df_final = df_final.select(*[c for c in cols if c in df_final.columns])\n",
    "\n",
    "df_typed = df_final \\\n",
    "    .withColumn(\"id_canal\", col(\"id_canal\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_canal\", col(\"nome_canal\").cast(StringType())) \\\n",
    "    .withColumn(\"canal_status\", col(\"canal_status\").cast(StringType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df_final.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "print(f\"Silver: {df_typed.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ae50e8-2e49-4954-be38-c1cdcef76c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_typed.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "silver_canais = spark.table(tgt_table)\n",
    "print(f\"Salvo: {tgt_table}\")\n",
    "display(silver_canais.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c24e3f-eece-426f-8090-9c64dcca6fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento: dm_clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5c4a03-e20e-4534-a1ff-b1b37ab26da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.dm_clientes\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_clientes\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df_src = spark.table(src_table)\n",
    "total_before = df_src.count()\n",
    "print(f\"Bronze: {total_before:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bba3aa-1850-473c-80a7-d2f79428db4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_src \\\n",
    "    .withColumn(\"id_cliente\", trim(safe_col(df_src, \"id_cliente\"))) \\\n",
    "    .withColumn(\"nome\", initcap(remove_accents_udf(normalize_text(safe_col(df_src, \"nome\"))))) \\\n",
    "    .withColumn(\"email\", lower(trim(safe_col(df_src, \"email\")))) \\\n",
    "    .withColumn(\"regiao\", initcap(remove_accents_udf(normalize_text(safe_col(df_src, \"regiao\"))))) \\\n",
    "    .withColumn(\"idade\", safe_cast_int(safe_col(df_src, \"idade\")))\n",
    "\n",
    "df_valid = df.filter(\n",
    "    (col(\"id_cliente\").isNotNull()) & (trim(col(\"id_cliente\")) != \"\") &\n",
    "    (col(\"nome\").isNotNull()) & (trim(col(\"nome\")) != \"\") &\n",
    "    (col(\"email\").isNotNull()) & (trim(col(\"email\")) != \"\") &\n",
    "    (col(\"email\").contains(\"@\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_cliente\").orderBy(col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df_valid.columns else lit(datetime.now()))\n",
    "\n",
    "df_dedup = df_valid.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "df_final = df_dedup.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "cols = [\"id_cliente\", \"nome\", \"email\", \"regiao\", \"idade\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df_final = df_final.select(*[c for c in cols if c in df_final.columns])\n",
    "\n",
    "df_typed = df_final \\\n",
    "    .withColumn(\"id_cliente\", col(\"id_cliente\").cast(StringType())) \\\n",
    "    .withColumn(\"nome\", col(\"nome\").cast(StringType())) \\\n",
    "    .withColumn(\"email\", col(\"email\").cast(StringType())) \\\n",
    "    .withColumn(\"regiao\", col(\"regiao\").cast(StringType())) \\\n",
    "    .withColumn(\"idade\", col(\"idade\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df_final.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "print(f\"Silver: {df_typed.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e03ed1-87b1-48f5-a1ad-d5b4beb81265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_typed.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "print(f\"Salvo: {tgt_table}\")\n",
    "display(spark.table(tgt_table).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f1dbb6-ef1b-41b4-90a4-0e1b2dac5178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento: ft_pesquisa_satisfacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560648ce-eea4-47c2-9e6b-f8fb51c3975e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_pesquisa_satisfacao\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_pesquisa_satisfacao\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df_src = spark.table(src_table)\n",
    "total_before = df_src.count()\n",
    "print(f\"Bronze: {total_before:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60725440-fd28-494f-bf85-2134adda6328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_src \\\n",
    "    .withColumn(\"id_pesquisa\", safe_cast_int(safe_col(df_src, \"id_pesquisa\"))) \\\n",
    "    .withColumn(\"id_chamado\", safe_cast_int(safe_col(df_src, \"id_chamado\"))) \\\n",
    "    .withColumn(\"nota_atendimento\", safe_cast_int(safe_col(df_src, \"nota_atendimento\")))\n",
    "\n",
    "df_valid = df.filter(\n",
    "    (col(\"id_pesquisa\").isNotNull()) & (col(\"id_pesquisa\") > 0) &\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"nota_atendimento\").isNotNull()) &\n",
    "    (col(\"nota_atendimento\") >= 1) & (col(\"nota_atendimento\") <= 5)\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_pesquisa\").orderBy(col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df_valid.columns else lit(datetime.now()))\n",
    "\n",
    "df_dedup = df_valid.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "df_final = df_dedup.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "cols = [\"id_pesquisa\", \"id_chamado\", \"nota_atendimento\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df_final = df_final.select(*[c for c in cols if c in df_final.columns])\n",
    "\n",
    "df_typed = df_final \\\n",
    "    .withColumn(\"id_pesquisa\", col(\"id_pesquisa\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nota_atendimento\", col(\"nota_atendimento\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df_final.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "print(f\"Silver: {df_typed.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581639cc-1793-4f84-82bd-320d56bc9301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_typed.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "print(f\"Salvo: {tgt_table}\")\n",
    "display(spark.table(tgt_table).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29694a22-9fa8-4a7c-be05-e4a9611063e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento: ft_chamados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06060d8a-22ce-4930-ada6-5a46d642b984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table_chamados = f\"{catalog_name}.{bronze_db_name}.ft_chamados\"\n",
    "src_table_motivos = f\"{catalog_name}.{bronze_db_name}.ft_motivos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_chamados\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table_chamados):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table_chamados}\")\n",
    "\n",
    "df_chamados_src = spark.table(src_table_chamados)\n",
    "df_motivos_src = spark.table(src_table_motivos)\n",
    "total_before = df_chamados_src.count()\n",
    "print(f\"Bronze: {total_before:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9aee25e-1c23-4ec0-830b-f82a714a0550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col_canal_norm = upper(remove_accents_udf(normalize_text(col(\"canal\"))))\n",
    "col_canal_final = (\n",
    "    when(col_canal_norm.like(\"%ESPECIALIZADO%\"), lit(\"Atendimento Especializado\"))\n",
    "    .when(col_canal_norm.like(\"%INICIAL%\"), lit(\"Atendimento Inicial\"))\n",
    "    .when(col_canal_norm.like(\"U%\"), lit(\"Ura\"))\n",
    "    .when(col_canal_norm.like(\"%BOT%\"), lit(\"Chatbot\"))\n",
    "    .when(col_canal_norm.like(\"%WEB%\"), lit(\"Web\"))\n",
    "    .when(col_canal_norm.like(\"%mail%\"), lit(\"Email\"))\n",
    "    .otherwise(col_canal_norm)\n",
    ")\n",
    "\n",
    "col_resolvido_norm = upper(remove_accents_udf(normalize_text(col(\"resolvido\"))))\n",
    "col_resolvido_final = when(col_resolvido_norm.like(\"S%\"), lit(\"Sim\")).when(col_resolvido_norm.like(\"N%\"), lit(\"Não\")).otherwise(col_resolvido_norm)\n",
    "\n",
    "df = df_chamados_src \\\n",
    "    .withColumn(\"canal\", col_canal_final) \\\n",
    "    .withColumn(\"resolvido\", col_resolvido_final) \\\n",
    "    .withColumn(\"motivo_norm\", upper(remove_accents_udf(normalize_text(col(\"motivo\")))))\n",
    "\n",
    "df_motivos = df_motivos_src.withColumn(\"nome_motivo_norm\", upper(remove_accents_udf(normalize_text(col(\"nome_motivo\")))))\n",
    "\n",
    "df_cross = df.alias(\"c\").crossJoin(df_motivos.alias(\"m\"))\n",
    "df_cross = df_cross.withColumn(\"similarity\", (1 - (levenshtein(col(\"motivo_norm\"), col(\"nome_motivo_norm\")) / F.greatest(length(col(\"motivo_norm\")), length(col(\"nome_motivo_norm\"))))) * 100)\n",
    "\n",
    "window = Window.partitionBy(\"c.id_chamado\").orderBy(col(\"similarity\").desc())\n",
    "df_best = df_cross.withColumn(\"rank\", F.row_number().over(window)).filter(col(\"rank\") == 1)\n",
    "\n",
    "df_final = df_best.alias(\"best\").join(silver_canais, col(\"best.canal\") == silver_canais.nome_canal, \"left\")\n",
    "\n",
    "df_final = df_final.join(\n",
    "    silver_chamados_hora.alias(\"h\"),\n",
    "    (col(\"best.id_chamado\") == col(\"h.id_chamado\")) & (col(\"best.id_cliente\") == col(\"h.id_cliente\")),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "df_final = df_final.select(\n",
    "    col(\"best.id_chamado\"),\n",
    "    col(\"best.id_cliente\"),\n",
    "    col(\"best.id_motivo\"),\n",
    "    col(\"best.motivo\"),\n",
    "    col(\"id_canal\"),\n",
    "    col(\"best.canal\"),\n",
    "    col(\"best.resolvido\"),\n",
    "    coalesce(col(\"h.hora_abertura_chamado_brasilia\"), col(\"best.hora_abertura_chamado\")).alias(\"hora_abertura_chamado\"),\n",
    "    when(col(\"best.hora_inicio_atendimento\") == \"igual a hora de abertura\", coalesce(col(\"h.hora_abertura_chamado_brasilia\"), col(\"best.hora_abertura_chamado\"))).otherwise(coalesce(col(\"h.hora_inicio_atendimento_brasilia\"), col(\"best.hora_inicio_atendimento\"))).alias(\"hora_inicio_atendimento\"),\n",
    "    coalesce(col(\"h.hora_finalizacao_atendimento_brasilia\"), col(\"best.hora_finalizacao_atendimento\")).alias(\"hora_finalizacao_atendimento\"),\n",
    "    col(\"best.id_atendente\"),\n",
    "    current_timestamp().alias(\"processed_timestamp\")\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    \"tempo_espera_minutos\",\n",
    "    when((col(\"hora_inicio_atendimento\").isNotNull()) & (col(\"hora_abertura_chamado\").isNotNull()), round((unix_timestamp(col(\"hora_inicio_atendimento\")) - unix_timestamp(col(\"hora_abertura_chamado\"))) / 60.0, 2)).otherwise(lit(None))\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    \"tempo_atendimento_minutos\",\n",
    "    when((col(\"hora_finalizacao_atendimento\").isNotNull()) & (col(\"hora_inicio_atendimento\").isNotNull()), round((unix_timestamp(col(\"hora_finalizacao_atendimento\")) - unix_timestamp(col(\"hora_inicio_atendimento\"))) / 60.0, 2)).otherwise(lit(None))\n",
    ")\n",
    "\n",
    "print(f\"Silver: {df_final.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b299f36e-f032-4890-9ee4-bcbc97084135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "print(f\"Salvo: {tgt_table}\")\n",
    "display(spark.table(tgt_table).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4bbada-98e9-4dcb-823e-6d189047767c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processamento: ft_custos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe11a00-df99-4fb4-a941-91b6c7ab856b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_custos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_custos\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df_src = spark.table(src_table)\n",
    "total_before = df_src.count()\n",
    "print(f\"Bronze: {total_before:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2cbaff-b382-4ea5-a180-09cf7b0350a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_src \\\n",
    "    .withColumn(\"id_chamado\", safe_cast_int(safe_col(df_src, \"id_chamado\"))) \\\n",
    "    .withColumn(\"id_custo\", safe_cast_int(safe_col(df_src, \"id_custo\"))) \\\n",
    "    .withColumn(\"custo\", regexp_replace(safe_col(df_src, \"custo\"), \"[^0-9,.-]\", \"\")) \\\n",
    "    .withColumn(\"custo\", regexp_replace(col(\"custo\"), \",\", \".\")) \\\n",
    "    .withColumn(\"custo\", col(\"custo\").cast(\"decimal(18,8)\"))\n",
    "\n",
    "df_valid = df.filter(\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"id_custo\").isNotNull()) & (col(\"id_custo\") > 0) &\n",
    "    (col(\"custo\").isNotNull()) & (col(\"custo\") >= 0)\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_chamado\", \"id_custo\").orderBy(col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df_valid.columns else lit(datetime.now()))\n",
    "\n",
    "df_dedup = df_valid.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "df_final = df_dedup.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "cols = [\"id_chamado\", \"id_custo\", \"custo\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df_final = df_final.select(*[c for c in cols if c in df_final.columns])\n",
    "\n",
    "df_typed = df_final \\\n",
    "    .withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_custo\", col(\"id_custo\").cast(IntegerType())) \\\n",
    "    .withColumn(\"custo\", col(\"custo\").cast(\"decimal(18,8)\")) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df_final.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "print(f\"Silver: {df_typed.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc86e50e-4d6f-4d04-815b-284ca85b3cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_typed.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "print(f\"Salvo: {tgt_table}\")\n",
    "display(spark.table(tgt_table).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce5c05e-2167-43bf-bcb0-da5d0ac2b44f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def safe_table_exists(spark, full_name: str) -> bool:\n",
    "    try:\n",
    "        spark.table(full_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def safe_col(df, name):\n",
    "    return col(name) if name in df.columns else lit(None).cast(StringType())\n",
    "\n",
    "def safe_cast_int(col_expr):\n",
    "    return when(\n",
    "        col_expr.isNotNull() & (trim(col_expr).cast(StringType()) != \"\"),\n",
    "        F.regexp_replace(trim(col_expr).cast(StringType()), r'[^\\d]', '').cast(IntegerType())\n",
    "    ).otherwise(None)\n",
    "\n",
    "def remove_accents(text_col):\n",
    "    return F.translate(\n",
    "        text_col,\n",
    "        \"áàãâäéèêëíìîïóòõôöúùûüçñÁÀÃÂÄÉÈÊËÍÌÎÏÓÒÕÔÖÚÙÛÜÇÑ\",\n",
    "        \"aaaaaeeeeiiiiooooouuuucnAAAAAEEEEIIIIOOOOOUUUUCN\"\n",
    "    )\n",
    "\n",
    "def normalize_text(col_expr):\n",
    "    return trim(regexp_replace(regexp_replace(col_expr, r'\\s+', ' '), r'[^\\x20-\\x7E\\u00C0-\\u00FF]', ''))\n",
    "\n",
    "def clean_brazil_timestamp(col_expr):\n",
    "    cleaned = regexp_replace(col_expr, r'[ ]', ' ')\n",
    "    cleaned = regexp_replace(cleaned, r'\\bàs\\b|\\bas\\b|\\bàs\\b', ' ')\n",
    "    cleaned = regexp_replace(cleaned, r'\\s+', ' ')\n",
    "    return trim(cleaned)\n",
    "\n",
    "def parse_to_brasilia_timezone(col_expr):\n",
    "    cleaned = clean_brazil_timestamp(col_expr)\n",
    "    parsed_naive = to_timestamp(cleaned, 'dd/MM/yyyy HH:mm:ss')\n",
    "    parsed_utc = F.to_utc_timestamp(parsed_naive, 'America/Sao_Paulo')\n",
    "    return F.from_utc_timestamp(parsed_utc, 'America/Sao_Paulo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3839bbe2-df79-40b6-ab61-502328322aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## FT_ATENDENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b3bc346-01dc-4146-b285-d43b6482a038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_atendentes\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_atendentes\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_atendente\", safe_cast_int(safe_col(df, \"id_atendente\"))) \\\n",
    "    .withColumn(\"nome_atendente\", initcap(remove_accents(normalize_text(safe_col(df, \"nome_atendente\"))))) \\\n",
    "    .withColumn(\n",
    "        \"nivel_atendimento\",\n",
    "        when(safe_cast_int(safe_col(df, \"nivel_atendimento\")).isin([1, 2]), safe_cast_int(safe_col(df, \"nivel_atendimento\"))).otherwise(None)\n",
    "    )\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_atendente\").isNotNull()) & (col(\"id_atendente\") > 0) &\n",
    "    (col(\"nome_atendente\").isNotNull()) & (trim(col(\"nome_atendente\")) != \"\") &\n",
    "    (col(\"nivel_atendimento\").isNotNull())\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_atendente\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_atendente\", \"nome_atendente\", \"nivel_atendimento\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_atendente\", col(\"id_atendente\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_atendente\", col(\"nome_atendente\").cast(StringType())) \\\n",
    "    .withColumn(\"nivel_atendimento\", col(\"nivel_atendimento\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"FT_ATENDENTES | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abcec36-bdfc-411c-a1a7-f0b7fdcdd650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## FT_CHAMADOS_HORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72669748-5a87-4211-81b0-cc35bc192fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_chamados_hora\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_chamados_hora\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_chamado\", safe_cast_int(safe_col(df, \"ID_Chamado\"))) \\\n",
    "    .withColumn(\"id_cliente\", trim(safe_col(df, \"ID_Cliente\"))) \\\n",
    "    .withColumn(\"hora_abertura_chamado_brasilia\", parse_to_brasilia_timezone(safe_col(df, \"Hora_Abertura_Chamado\"))) \\\n",
    "    .withColumn(\"hora_inicio_atendimento_brasilia\", parse_to_brasilia_timezone(safe_col(df, \"Hora_Inicio_Atendimento\"))) \\\n",
    "    .withColumn(\"hora_finalizacao_atendimento_brasilia\", parse_to_brasilia_timezone(safe_col(df, \"Hora_Finalizacao_Atendimento\")))\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"id_cliente\").isNotNull()) & (length(col(\"id_cliente\")) > 0) &\n",
    "    (col(\"hora_abertura_chamado_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_inicio_atendimento_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_finalizacao_atendimento_brasilia\").isNotNull()) &\n",
    "    (col(\"hora_abertura_chamado_brasilia\") <= col(\"hora_inicio_atendimento_brasilia\")) &\n",
    "    (col(\"hora_inicio_atendimento_brasilia\") <= col(\"hora_finalizacao_atendimento_brasilia\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_chamado\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_chamado\", \"id_cliente\", \"hora_abertura_chamado_brasilia\", \"hora_inicio_atendimento_brasilia\", \"hora_finalizacao_atendimento_brasilia\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_cliente\", col(\"id_cliente\").cast(StringType())) \\\n",
    "    .withColumn(\"hora_abertura_chamado_brasilia\", col(\"hora_abertura_chamado_brasilia\").cast(TimestampType())) \\\n",
    "    .withColumn(\"hora_inicio_atendimento_brasilia\", col(\"hora_inicio_atendimento_brasilia\").cast(TimestampType())) \\\n",
    "    .withColumn(\"hora_finalizacao_atendimento_brasilia\", col(\"hora_finalizacao_atendimento_brasilia\").cast(TimestampType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"FT_CHAMADOS_HORA | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7c12c3-d622-40f8-993d-8eb1283e1a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DM_MOTIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90798f7b-05ba-4ec3-95cc-d1f8672ceb3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_motivos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_motivos\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_motivo\", safe_cast_int(safe_col(df, \"id_motivo\"))) \\\n",
    "    .withColumn(\"nome_motivo\", safe_col(df, \"nome_motivo\")) \\\n",
    "    .withColumn(\n",
    "        \"categoria\",\n",
    "        when(lower(col(\"nome_motivo\")).like(\"%fatura%\") | lower(col(\"nome_motivo\")).like(\"%limite%\") | lower(col(\"nome_motivo\")).like(\"%contrato%\") | lower(col(\"nome_motivo\")).like(\"%pagamento%\") | lower(col(\"nome_motivo\")).like(\"%divida%\") | lower(col(\"nome_motivo\")).like(\"%renegocia%\"), lit(\"Financeiro\"))\n",
    "        .when(lower(col(\"nome_motivo\")).like(\"%cartao%\") | lower(col(\"nome_motivo\")).like(\"%bloqueio%\") | lower(col(\"nome_motivo\")).like(\"%desbloqueio%\") | lower(col(\"nome_motivo\")).like(\"%compra%\") | lower(col(\"nome_motivo\")).like(\"%adicional%\"), lit(\"Cartão\"))\n",
    "        .when(lower(col(\"nome_motivo\")).like(\"%dados%\") | lower(col(\"nome_motivo\")).like(\"%cadastra%\") | lower(col(\"nome_motivo\")).like(\"%telefone%\") | lower(col(\"nome_motivo\")).like(\"%email%\") | lower(col(\"nome_motivo\")).like(\"%agencia%\") | lower(col(\"nome_motivo\")).like(\"%conta%\") | lower(col(\"nome_motivo\")).like(\"%endereco%\"), lit(\"Cadastral\"))\n",
    "        .when(lower(col(\"nome_motivo\")).like(\"%app%\") | lower(col(\"nome_motivo\")).like(\"%aplicativo%\") | lower(col(\"nome_motivo\")).like(\"%site%\") | lower(col(\"nome_motivo\")).like(\"%chatbot%\") | lower(col(\"nome_motivo\")).like(\"%ura%\") | lower(col(\"nome_motivo\")).like(\"%problema%\") | lower(col(\"nome_motivo\")).like(\"%erro%\"), lit(\"Atendimento\"))\n",
    "        .when(lower(col(\"nome_motivo\")).like(\"%ponto%\") | lower(col(\"nome_motivo\")).like(\"%beneficio%\") | lower(col(\"nome_motivo\")).like(\"%programa%\"), lit(\"Benefícios\"))\n",
    "        .otherwise(lit(\"Desconhecida\"))\n",
    "    ) \\\n",
    "    .withColumn(\"criticidade\", when(col(\"criticidade\") == lit(\"Media\"), lit(\"Média\")).otherwise(col(\"criticidade\")))\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_motivo\").isNotNull()) & (col(\"id_motivo\") > 0) &\n",
    "    (col(\"nome_motivo\").isNotNull()) & (trim(col(\"nome_motivo\")) != \"\")\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_motivo\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_motivo\", \"nome_motivo\", \"categoria\", \"criticidade\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_motivo\", col(\"id_motivo\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_motivo\", col(\"nome_motivo\").cast(StringType())) \\\n",
    "    .withColumn(\"categoria\", col(\"categoria\").cast(StringType())) \\\n",
    "    .withColumn(\"criticidade\", col(\"criticidade\").cast(StringType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"DM_MOTIVOS | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e236295-9337-4f8f-b79c-4a245ce44c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DM_CANAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725e8b66-2d67-4459-bc73-b774e06d00aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.dm_canais\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_canais\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"nome_canal\", initcap(remove_accents(normalize_text(safe_col(df, \"nome_canal\"))))) \\\n",
    "    .withColumn(\"canal_status\", initcap(remove_accents(normalize_text(safe_col(df, \"canal_status\"))))) \\\n",
    "    .withColumn(\n",
    "        \"canal_status\",\n",
    "        when(col(\"canal_status\") == lit(\"Invativo\"), lit(\"Ativo\"))\n",
    "        .when(col(\"nome_canal\") == \"Web\", lit(\"Inativo\"))\n",
    "        .otherwise(col(\"canal_status\"))\n",
    "    )\n",
    "\n",
    "w_id = Window.orderBy(\"canal_status\")\n",
    "df = df.withColumn(\"id_canal\", row_number().over(w_id))\n",
    "\n",
    "df = df.filter((col(\"nome_canal\").isNotNull()) & (trim(col(\"nome_canal\")) != \"\"))\n",
    "\n",
    "w = Window.partitionBy(\"nome_canal\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_canal\", \"nome_canal\", \"canal_status\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_canal\", col(\"id_canal\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nome_canal\", col(\"nome_canal\").cast(StringType())) \\\n",
    "    .withColumn(\"canal_status\", col(\"canal_status\").cast(StringType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"DM_CANAIS | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429a71cc-d440-45c5-9dd0-51d613facf27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DM_CLIENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e8e2cd-aa6f-4c4c-a94f-ce10b30439ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.dm_clientes\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.dm_clientes\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_cliente\", trim(safe_col(df, \"id_cliente\"))) \\\n",
    "    .withColumn(\"nome\", initcap(remove_accents(normalize_text(safe_col(df, \"nome\"))))) \\\n",
    "    .withColumn(\"email\", lower(trim(safe_col(df, \"email\")))) \\\n",
    "    .withColumn(\"regiao\", initcap(remove_accents(normalize_text(safe_col(df, \"regiao\"))))) \\\n",
    "    .withColumn(\"idade\", safe_cast_int(safe_col(df, \"idade\")))\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_cliente\").isNotNull()) & (trim(col(\"id_cliente\")) != \"\") &\n",
    "    (col(\"nome\").isNotNull()) & (trim(col(\"nome\")) != \"\") &\n",
    "    (col(\"email\").isNotNull()) & (trim(col(\"email\")) != \"\") & (col(\"email\").contains(\"@\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_cliente\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_cliente\", \"nome\", \"email\", \"regiao\", \"idade\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_cliente\", col(\"id_cliente\").cast(StringType())) \\\n",
    "    .withColumn(\"nome\", col(\"nome\").cast(StringType())) \\\n",
    "    .withColumn(\"email\", col(\"email\").cast(StringType())) \\\n",
    "    .withColumn(\"regiao\", col(\"regiao\").cast(StringType())) \\\n",
    "    .withColumn(\"idade\", col(\"idade\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"DM_CLIENTES | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b0e781-1764-4ee8-bbac-daec65ede522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## FT_PESQUISA_SATISFACAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf501c3-7dec-4339-9b6b-1f21cf79a8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_pesquisa_satisfacao\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_pesquisa_satisfacao\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_pesquisa\", safe_cast_int(safe_col(df, \"id_pesquisa\"))) \\\n",
    "    .withColumn(\"id_chamado\", safe_cast_int(safe_col(df, \"id_chamado\"))) \\\n",
    "    .withColumn(\"nota_atendimento\", safe_cast_int(safe_col(df, \"nota_atendimento\")))\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_pesquisa\").isNotNull()) & (col(\"id_pesquisa\") > 0) &\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"nota_atendimento\").isNotNull()) & (col(\"nota_atendimento\") >= 1) & (col(\"nota_atendimento\") <= 5)\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_pesquisa\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_pesquisa\", \"id_chamado\", \"nota_atendimento\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_pesquisa\", col(\"id_pesquisa\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"nota_atendimento\", col(\"nota_atendimento\").cast(IntegerType())) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"FT_PESQUISA_SATISFACAO | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05309e6e-f8f4-4f9c-b394-50d3b4c670b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## FT_CHAMADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7a3175-93f2-402b-a202-e3d2f36759cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_chamados = f\"{catalog_name}.{bronze_db_name}.ft_chamados\"\n",
    "path_motivos = f\"{catalog_name}.{bronze_db_name}.ft_motivos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_chamados\"\n",
    "src_canais = f\"{catalog_name}.{silver_db_name}.dm_canais\"\n",
    "src_hora = f\"{catalog_name}.{silver_db_name}.ft_chamados_hora\"\n",
    "\n",
    "if not safe_table_exists(spark, path_chamados):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {path_chamados}\")\n",
    "\n",
    "df = spark.table(path_chamados)\n",
    "total_before = df.count()\n",
    "\n",
    "df_motivos = spark.table(path_motivos).withColumn(\"nome_motivo_norm\", upper(remove_accents(normalize_text(col(\"nome_motivo\")))))\n",
    "df_canais = spark.table(src_canais)\n",
    "df_hora = spark.table(src_hora)\n",
    "\n",
    "col_canal_final = (\n",
    "    when(upper(remove_accents(normalize_text(col(\"canal\")))).like(\"%ESPECIALIZADO%\"), lit(\"Atendimento Especializado\"))\n",
    "    .when(upper(remove_accents(normalize_text(col(\"canal\")))).like(\"%INICIAL%\"), lit(\"Atendimento Inicial\"))\n",
    "    .when(upper(remove_accents(normalize_text(col(\"canal\")))).like(\"U%\"), lit(\"Ura\"))\n",
    "    .when(upper(remove_accents(normalize_text(col(\"canal\")))).like(\"%BOT%\"), lit(\"Chatbot\"))\n",
    "    .when(upper(remove_accents(normalize_text(col(\"canal\")))).like(\"%WEB%\"), lit(\"Web\"))\n",
    "    .when(upper(remove_accents(normalize_text(col(\"canal\")))).like(\"%mail%\"), lit(\"Email\"))\n",
    "    .otherwise(upper(remove_accents(normalize_text(col(\"canal\")))))\n",
    ")\n",
    "\n",
    "col_resolvido_final = (\n",
    "    when(upper(remove_accents(normalize_text(col(\"resolvido\")))).like(\"S%\"), lit(\"Sim\"))\n",
    "    .when(upper(remove_accents(normalize_text(col(\"resolvido\")))).like(\"N%\"), lit(\"Não\"))\n",
    "    .otherwise(upper(remove_accents(normalize_text(col(\"resolvido\")))))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"canal\", col_canal_final).withColumn(\"resolvido\", col_resolvido_final)\n",
    "df = df.withColumn(\"motivo_norm\", upper(remove_accents(normalize_text(col(\"motivo\")))))\n",
    "\n",
    "df_cross = df.alias(\"c\").crossJoin(df_motivos.alias(\"m\"))\n",
    "df_cross = df_cross.withColumn(\n",
    "    \"similarity\",\n",
    "    (1 - (levenshtein(col(\"motivo_norm\"), col(\"nome_motivo_norm\")) / F.greatest(length(col(\"motivo_norm\")), length(col(\"nome_motivo_norm\"))))) * 100\n",
    ")\n",
    "\n",
    "w_sim = Window.partitionBy(\"c.id_chamado\").orderBy(col(\"similarity\").desc())\n",
    "df_best = df_cross.withColumn(\"rank\", F.row_number().over(w_sim)).filter(col(\"rank\") == 1)\n",
    "\n",
    "df_final = df_best.join(df_canais, df_best.canal == df_canais.nome_canal, \"left\") \\\n",
    "    .join(df_hora.alias(\"h\"), [col(\"c.id_chamado\") == col(\"h.id_chamado\"), col(\"c.id_cliente\") == col(\"h.id_cliente\")], \"left\")\n",
    "\n",
    "df_final = df_final.select(\n",
    "    col(\"c.id_chamado\"),\n",
    "    col(\"c.id_cliente\"),\n",
    "    col(\"m.id_motivo\"),\n",
    "    col(\"m.nome_motivo\").alias(\"motivo\"),\n",
    "    col(\"id_canal\"),\n",
    "    col(\"c.canal\"),\n",
    "    col(\"c.resolvido\"),\n",
    "    coalesce(col(\"h.hora_abertura_chamado_brasilia\"), col(\"c.hora_abertura_chamado\")).alias(\"hora_abertura_chamado\"),\n",
    "    when(col(\"c.hora_inicio_atendimento\") == \"igual a hora de abertura\", coalesce(col(\"h.hora_abertura_chamado_brasilia\"), col(\"c.hora_abertura_chamado\")))\n",
    "        .otherwise(coalesce(col(\"h.hora_inicio_atendimento_brasilia\"), col(\"c.hora_inicio_atendimento\"))).alias(\"hora_inicio_atendimento\"),\n",
    "    coalesce(col(\"h.hora_finalizacao_atendimento_brasilia\"), col(\"c.hora_finalizacao_atendimento\")).alias(\"hora_finalizacao_atendimento\"),\n",
    "    col(\"c.tempo_espera\"),\n",
    "    col(\"c.tempo_atendimento\"),\n",
    "    col(\"c.id_atendente\"),\n",
    "    current_timestamp().alias(\"processed_timestamp\")\n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    \"tempo_espera\",\n",
    "    when((col(\"hora_inicio_atendimento\").isNotNull()) & (col(\"hora_abertura_chamado\").isNotNull()),\n",
    "        round((unix_timestamp(col(\"hora_inicio_atendimento\")) - unix_timestamp(col(\"hora_abertura_chamado\"))) / 60.0, 2)\n",
    "    ).otherwise(lit(None))\n",
    ").withColumn(\n",
    "    \"tempo_atendimento\",\n",
    "    when((col(\"hora_finalizacao_atendimento\").isNotNull()) & (col(\"hora_inicio_atendimento\").isNotNull()),\n",
    "        round((unix_timestamp(col(\"hora_finalizacao_atendimento\")) - unix_timestamp(col(\"hora_inicio_atendimento\"))) / 60.0, 2)\n",
    "    ).otherwise(lit(None))\n",
    ").withColumnRenamed(\"tempo_espera\", \"tempo_espera_minutos\").withColumnRenamed(\"tempo_atendimento\", \"tempo_atendimento_minutos\")\n",
    "\n",
    "df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"FT_CHAMADOS | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf130ff-6e0f-4a07-a85f-c4bc26a9cc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## FT_CUSTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38659bf0-8329-4382-84a4-687dfa9679d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = f\"{catalog_name}.{bronze_db_name}.ft_custos\"\n",
    "tgt_table = f\"{catalog_name}.{silver_db_name}.ft_custos\"\n",
    "\n",
    "if not safe_table_exists(spark, src_table):\n",
    "    raise RuntimeError(f\"Tabela não encontrada: {src_table}\")\n",
    "\n",
    "df = spark.table(src_table)\n",
    "total_before = df.count()\n",
    "\n",
    "df = df.withColumn(\"id_chamado\", safe_cast_int(safe_col(df, \"id_chamado\"))) \\\n",
    "    .withColumn(\"id_custo\", safe_cast_int(safe_col(df, \"id_custo\"))) \\\n",
    "    .withColumn(\"custo\", regexp_replace(regexp_replace(regexp_replace(safe_col(df, \"custo\"), \"[^0-9,.-]\", \"\"), \",\", \".\"), r'^\\.$', \"0\").cast(\"decimal(18,8)\"))\n",
    "\n",
    "df = df.filter(\n",
    "    (col(\"id_chamado\").isNotNull()) & (col(\"id_chamado\") > 0) &\n",
    "    (col(\"id_custo\").isNotNull()) & (col(\"id_custo\") > 0) &\n",
    "    (col(\"custo\").isNotNull()) & (col(\"custo\") >= 0)\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"id_chamado\", \"id_custo\").orderBy(\n",
    "    col(\"ingestion_timestamp\").desc_nulls_last() if \"ingestion_timestamp\" in df.columns else lit(datetime.now())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "df = df.withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "final_cols = [\"id_chamado\", \"id_custo\", \"custo\", \"processed_timestamp\", \"ingestion_timestamp\"]\n",
    "df = df.select(*[c for c in final_cols if c in df.columns])\n",
    "df = df.withColumn(\"id_chamado\", col(\"id_chamado\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_custo\", col(\"id_custo\").cast(IntegerType())) \\\n",
    "    .withColumn(\"custo\", col(\"custo\").cast(\"decimal(18,8)\")) \\\n",
    "    .withColumn(\"processed_timestamp\", col(\"processed_timestamp\").cast(TimestampType())) \\\n",
    "    .withColumn(\"ingestion_timestamp\", col(\"ingestion_timestamp\").cast(TimestampType()) if \"ingestion_timestamp\" in df.columns else lit(None).cast(TimestampType()))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt_table)\n",
    "final = spark.table(tgt_table)\n",
    "\n",
    "print(f\"FT_CUSTOS | Bronze: {total_before:,} | Silver: {final.count():,} | Taxa: {(final.count()/total_before*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3582e8aa-02ab-418d-a5ce-e17934548d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Resumo da Transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d36f48a-ac09-4943-846d-7d18cd2b8cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"ft_atendentes\",\n",
    "    \"ft_chamados_hora\",\n",
    "    \"dm_motivos\",\n",
    "    \"dm_canais\",\n",
    "    \"dm_clientes\",\n",
    "    \"ft_pesquisa_satisfacao\",\n",
    "    \"ft_chamados\",\n",
    "    \"ft_custos\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSFORMAÇÃO BRONZE → SILVER FINALIZADA\")\n",
    "print(\"=\"*80)\n",
    "for table in tables:\n",
    "    try:\n",
    "        tgt = f\"{catalog_name}.{silver_db_name}.{table}\"\n",
    "        count = spark.table(tgt).count()\n",
    "        print(f\"✓ {table:30} | Registros: {count:>12,}\")\n",
    "    except:\n",
    "        print(f\"✗ {table:30} | Erro ao validar\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
